{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLLwAepk1A3J"
   },
   "source": [
    "## Notebook 1\n",
    "\n",
    "This  note should be run twice, first time without any tranformations and second time with the transformations. \n",
    "The user can find the code in the \n",
    "This notebook applies the transformations/ data preprocessing on the datasets and calculates the multi CVI rank correlation again and concatenates the new informations in the Knowledge space. The knowledge space/ meta data includes the extracted meta features and applied transformations, combination of the multi CVIs and the correlation score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "The libraries required for the experiment is in the following code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required libraries. \n",
    "#install deap if not installed \n",
    "\n",
    "#pip install deap\n",
    "import itertools\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering, Birch\n",
    "import warnings\n",
    "from sklearn.metrics import normalized_mutual_info_score, davies_bouldin_score\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from metafeatures import Meta\n",
    "from deap import base\n",
    "from deap import creator, tools\n",
    "from scipy.stats import spearmanr\n",
    "from cvi import Validation\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import glob\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from cvi import Validation\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the multi CVI functions\n",
    "The following cell holds the definition of the the few Multi CVIs that were either used for the experiments. Some of them were not used for the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiFH_2RZ1A3P"
   },
   "outputs": [],
   "source": [
    "#C-index CVI\n",
    "def c_index(data, class_label):\n",
    "    \"\"\"\n",
    "        The C-Index, a measure of dispersionn\n",
    "    \"\"\"\n",
    "    sw = 0\n",
    "    nw = 0\n",
    "    numCluster = max(class_label) + 1\n",
    "    data_matrix = np.asmatrix(data).astype(np.float)\n",
    "    \n",
    "    # iterate through all the clusters\n",
    "    for i in range(numCluster):\n",
    "        indices = [t for t, x in enumerate(class_label) if x == i]\n",
    "        clusterMember = data_matrix[indices, :]\n",
    "        # compute distance of every pair of points\n",
    "        list_clusterDis = distance.pdist(clusterMember)\n",
    "        sw = sw + sum(list_clusterDis)\n",
    "        nw = nw + len(list_clusterDis)\n",
    "    # compute the pairwise distance of the whole dataset\n",
    "    list_dataDis = distance.pdist(data_matrix)\n",
    "    # compute smin\n",
    "    sortedList = sorted(list_dataDis)\n",
    "    smin = sum(sortedList[0:nw])\n",
    "    # compute smax\n",
    "    sortedList = sorted(list_dataDis, reverse=True)\n",
    "    smax = sum(sortedList[0:nw])\n",
    "    \n",
    "    # compute the score\n",
    "    return (sw - smin) / (smax - smin)\n",
    "\n",
    "\n",
    "#i-index CVI  \n",
    "def i_index(data, class_label):\n",
    "      \"\"\"\n",
    "        The I index, a measure of compactness.\n",
    "      \"\"\"\n",
    "      \n",
    "      normClusterSum = 0\n",
    "      normDatasetSum = 0\n",
    "      list_centers = []\n",
    "\n",
    "      data_matrix = np.asmatrix(data).astype(np.float)\n",
    "      # compute the number of clusters and attribute\n",
    "      attributes = len(data_matrix[0])\n",
    "      numCluster = max(class_label) + 1\n",
    "      # compute the center of the dataset\n",
    "      dataCenter = np.mean(data_matrix, 0)\n",
    "      # iterate through all the clusters\n",
    "      for i in range(numCluster):\n",
    "          indices = [t for t, x in enumerate(class_label) if x == i]\n",
    "          clusterMember = data_matrix[indices, :]\n",
    "          # compute the center of the cluster\n",
    "          clusterCenter = np.mean(clusterMember, 0)\n",
    "          list_centers.append(np.asarray(clusterCenter))\n",
    "          # compute the norm for every member in the cluster with cluster center and dataset center\n",
    "          for member in clusterMember:\n",
    "              normClusterSum += distance.euclidean(member, clusterCenter)\n",
    "              normDatasetSum += distance.euclidean(member, dataCenter)\n",
    "      # compute the max distance between cluster centers\n",
    "      list_centers = np.concatenate(list_centers, axis=0)\n",
    "      maxCenterDis = max(distance.pdist(list_centers))\n",
    "      \n",
    "      # compute the fitness      \n",
    "      return math.pow(((normDatasetSum * maxCenterDis) / (normClusterSum * numCluster)), attributes)\n",
    "    \n",
    "\n",
    "#Banfel Rafery CVI\n",
    "def banfeld_raferty(data, class_label):\n",
    "    \"\"\" Banfeld-Raferty index is the weighted sum of the logarithms\n",
    "         of the traces of the variance-covariance matrix of each cluster\n",
    "         \n",
    "        Weighted sum of the logarithms of the traces of the variance-covariance matrix of each cluster\n",
    "        \n",
    "        OBJECTIVE: MIN\n",
    "    \"\"\"\n",
    "\n",
    "    sum_total = 0\n",
    "    num_cluster = max(class_label) + 1\n",
    "    data_matrix = np.asmatrix(data).astype(np.float)\n",
    "    \n",
    "    # iterate through all the clusters\n",
    "    for i in range(num_cluster):\n",
    "        sum_dis = 0\n",
    "        indices = [t for t, x in enumerate(class_label) if x == i]\n",
    "        cluster_member = data_matrix[indices, :]\n",
    "\n",
    "        # compute the center of the cluster\n",
    "        cluster_center = np.mean(cluster_member, 0)\n",
    "\n",
    "        # iterate through all the members\n",
    "        for member in cluster_member:\n",
    "            sum_dis += distance.euclidean(member, cluster_center) ** 2\n",
    "\n",
    "        op = sum_dis / len(indices)\n",
    "        if op <= 0:\n",
    "            # Cannot calculate Banfeld_Raferty, due to an undefined value\n",
    "            continue\n",
    "        else:\n",
    "            sum_total += len(indices) * math.log(sum_dis / len(indices))\n",
    "\n",
    "    return sum_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFm7JV5h1A3R"
   },
   "source": [
    "### Setting configuration\n",
    "The following cell includes setting the hyperparameter range including the clustering algorithm. \n",
    "For the sake of the experiment, we used three clustering algorithm, KMeans, Agglomerative, and Birch. \\\\\n",
    "\n",
    "The each algorithm was applied sperately at one time. User can comment/ uncomment to use parts of the code in the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8q-flT01A3T"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Generates different cluster configurations from the KMeans, Agglomerated, Birch Clustering algorithm, \n",
    "    modifying hyper-parameters: n_clusters, \n",
    "\"\"\"\n",
    "\n",
    "#keeping the configuration in a list\n",
    "configs = list()\n",
    "\"\"\"\n",
    "#Agglo\n",
    "for i in np.arange(2, 20, 1):\n",
    "    configs.append(AgglomerativeClustering( n_clusters=i))\n",
    "\n",
    "#for j in np.arange(0.5, 1.5, 0.5):\n",
    "#    configs.append(AgglomerativeClustering(distance_threshold=j, n_clusters=None))\n",
    "   \n",
    "\"\"\"\n",
    "\n",
    "##K-Means\n",
    "for i in list(range(100, 101,1)):\n",
    "    #configs.append(AgglomerativeClustering(n_clusters=i))\n",
    "\n",
    "    for j in list(range(2, 20,1)):\n",
    "        configs.append(KMeans(n_clusters=j, max_iter=i)) #user can use other clustering algorithm for each phase here\n",
    "\n",
    "\"\"\"\n",
    "#Birch\n",
    "for i in np.arange(2, 20, 1):\n",
    "    configs.append(Birch( n_clusters=i))\n",
    "  \n",
    "#Spectral####not performed\n",
    "for i in np.arange(2, 20, 1):\n",
    "    configs.append(SpectralClustering( n_clusters=i))\n",
    "\"\"\"\n",
    "#we need the length of the configuration for ranking later\n",
    "len(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming, clustering, and calculating multi CVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UMnEbKT1A3U",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Multi CVI uses DEAP, an Evolutionary Algorithm implementation we will use in later tasks\n",
    "    Goal here is to sort cluster configurations with multi-objective functions, using the Non-dominated sorting algorithm II (NSGA-2)\n",
    "    More about  NSGA-2 here: https://ieeexplore.ieee.org/document/6600851\n",
    "\n",
    "    Parameter Definition REQUIRED\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#initializing the preprocessing methods\n",
    "kbin = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "norm_l1=Normalizer(norm='l1')\n",
    "norm_l2=Normalizer(norm='l2')\n",
    "std_zscore=StandardScaler()\n",
    "scaling_minmax=MinMaxScaler()\n",
    "#replacing_mean=SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#replacing_median=SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "\"\"\"\n",
    "    This the part where the user needs to disable the transformations to prevent transforming the data first \n",
    "    time, and then cluster and calculate the multi CVI correlation score ones. \n",
    "    \n",
    "    Then for second time, the user needs to enable the transformations, to transform the data and cluster and\n",
    "    calculate the multi CVI correlation score again. \n",
    "    \n",
    "    Each time I saved the dataframe and later cocatenated them in other notebook for analysis. \n",
    "    \n",
    "    For transofrmaiton, keeping the preprocessing methods in a list so that we can use a for loop later tto iterate over them\n",
    "    Later i manually used mean and median manaully as the packageeee was giving some error. \n",
    "\"\"\"\n",
    "\n",
    "transformations = [kbin, norm_l1, norm_l2, scaling_minmax, std_zscore]#, replacing_mean, replacing_median]\n",
    "\n",
    "\"\"\"\n",
    "    This list is for creating all the combinations of transformations in a pair, i.e. [kbin, norm_l1], [norm_l1, norm_l2]\n",
    "    we apply this to create an additional knowledge applying two transformations at once on each dataset to check\n",
    "    if applying two preprocessing method improves/ degrades the result\n",
    "\"\"\"\n",
    "tr_list=list(combinations([0,1,2,3, 4],2))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Initializing the knowledge base dataframe where we will store the information, meta features of the datasets\n",
    "\"\"\"\n",
    "\n",
    "df_meta=pd.DataFrame(columns=['name', 'instances', 'attributtes', 'classes', 'cat_att', 'cont_att', 'mean', 'sd', 'var', 'kurtosis', 'skewness', 'MD6', 'MD7', 'MD8', 'MD9',\n",
    "       'MD10', 'MD11', 'MD12', 'MD13', 'MD14', 'MD15', 'MD16', 'MD17', 'MD18',\n",
    "       'MD19', 'dataset', 'best_cluster_setting', 'transformation','multi_cvi', 'multi_cvi_correlation_score'])\n",
    "\n",
    "#Path to the datasets\n",
    "files=glob.glob(r\"/Users/hasan.tanvir/Documents/Thesis/Data/clustering-benchmark-master/src/main/resources/datasets/artificial/*.arff\")\n",
    "print(len(files))\n",
    "\n",
    "for t in transformations:\n",
    "\n",
    "    \n",
    "\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        \n",
    "        #read the dataset\n",
    "        data = arff.loadarff(file)\n",
    "        df = pd.DataFrame(data[0])\n",
    "        df.columns = [x.lower() for x in df.columns]\n",
    "        cols=df.columns\n",
    "        num_cols = df._get_numeric_data().columns\n",
    "        df=df.dropna() \n",
    "        \n",
    "        #check if the dataset has column class & some preprocessing\n",
    "        if df.shape[1]>2:\n",
    "            df['class'] = df['class'].str.decode('utf-8') \n",
    "            df['class']=df['class'].replace('noise','3',regex=True)\n",
    "\n",
    "            try:# int(df['class'][0]): #for classes where class labels are integer\n",
    "                df['class']=df['class'].astype(str).astype(int)\n",
    "                \n",
    "                #getting the file name\n",
    "                new_file = file.split('/')[-1].split('.')[0]\n",
    "                #print(new_file)\n",
    "\n",
    "            except: #for classes where the class labels are not integer\n",
    "                df_cols=df.columns\n",
    "                encoder = OrdinalEncoder()\n",
    "                # transform data\n",
    "                #result = encoder.fit_transform(df)\n",
    "                df=pd.DataFrame(result, columns=df_cols)\n",
    "\n",
    "                new_file = file.split('/')[-1].split('.')[0]\n",
    "                print(new_file)\n",
    "#\"\"\"\n",
    "#    Using the file name to extract metafeature, for extracting meta features, I used the csmartml package\n",
    "#    and the package requires the files to be read in CSV. So I converted the files in CSV beforehand so that we \n",
    "#    can perform this step\n",
    "#\"\"\"\n",
    "        df1=Meta(\"csmartml/datasets/\"+new_file+\".csv\").extract_metafeatures(meta_type=\"distance\")\n",
    "\n",
    "        \n",
    "        meta_dict=df1.to_dict('dict')   \n",
    "#\"\"\"\n",
    "#    This is the part where we apply mean and median manually as the package give some kind of erros. \n",
    "#    Since the dataset does not have empty/ null/ NA values, so far I guess, I manually, randomly remove 20%\n",
    "#    of the data and then impute those with mean and media.     \n",
    "#\"\"\"\n",
    "\n",
    "        #for col in df.columns:\n",
    "        #    df.loc[df.sample(frac=0.2).index, col] = np.nan \n",
    "\n",
    "        #df.apply(lambda x: x.fillna(x.mean(), inplace=True),axis=0)\n",
    "\n",
    "        X=df[df.columns.difference(['class'])].values\n",
    "        y=df['class'].values\n",
    "\n",
    "\n",
    "        #df=df.dropna() \n",
    "        #print(X)\n",
    "        X=t.fit_transform(df[df.columns.difference(['class'])].values)\n",
    "\n",
    "#\"\"\"\n",
    "#    The following section directly comes from SmartML homework, thatI received from Hudson. This is where\n",
    "#    the multi CVI are ranked using NSGA-2 and the corraltion scores are calculated. I tried to use/ reproduce\n",
    "#    it dunamically, but that seemed to be too much of work hence, I refrained. \n",
    "    \n",
    "#    The only change I made is added one more CVI to make it a combination of 3 and added the optimization \n",
    "#    objective in the objecitve function to define the objective of the new CVI. I believe this part won't be \n",
    "#    needed for the cSmartML project if you want to integrate preprocessing part in the tool later. \n",
    "#\"\"\"\n",
    "        def fitness_function(individual):\n",
    "            clustering = individual[0].fit(X)\n",
    "            #print(individual[0])\n",
    "            labels = clustering.labels_\n",
    "\n",
    "            #if len(set(labels))==1:\n",
    "                #return 0\n",
    "            #print('clus',set(labels))\n",
    "            #print(labels)\n",
    "            #print('X', len(X))\n",
    "            return Validation(X, df.values,labels).s_dbw(), davies_bouldin_score(X, labels),banfeld_raferty(X, labels)\n",
    "\n",
    "        def initIndividual(icls, content):\n",
    "            return icls([content])\n",
    "\n",
    "        def initPopulation(pcls, ind_init, poplist):\n",
    "            return pcls(ind_init(c) for c in poplist)\n",
    "\n",
    "        # Think of creator as an abstraction for classes that can be modified for evolutionary algorithms.\n",
    "        # Eg. in this case is the base Fitness class\n",
    "        # weights = tuple defining objective of CVI, max: 1 or min: -1\n",
    "        # weight should be fixed according to the objective function of the CVI, respectively\n",
    "        creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0, -1.0, -1.0)) \n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "        # Toolbox contains specific functions for evolutionary operations\n",
    "        # Read more: https://deap.readthedocs.io/en/master/api/base.html?highlight=toolbox#toolbox\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"individual\", initIndividual, creator.Individual)\n",
    "        toolbox.register(\"population\", initPopulation, list, toolbox.individual, configs)\n",
    "        toolbox.register(\"evaluate\", fitness_function)\n",
    "        toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "#        \"\"\"\n",
    "#            Rank configuration by multiple CVIs\n",
    "#        \"\"\"\n",
    "        pop = toolbox.population()\n",
    "\n",
    "        pop_dict = {} # Numbering configurations so it's easy to compare their ranks\n",
    "        for i, j in enumerate(pop):\n",
    "            pop_dict[i] = j\n",
    "\n",
    "        invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        #Fix the value for toolbox.select(pop, N), the N should be the numbe rof configurations we have from \n",
    "        #the previous cell, could have been dynamic, but I missed :) \n",
    "        \n",
    "        nsga_sorted = toolbox.select(pop,18)\n",
    "        multi_cvi_rank = [list(pop_dict.values()).index(i) for i in nsga_sorted]\n",
    "        multi_cvi_rank\n",
    "        #print(pop_dict[multi_cvi_rank[0]])\n",
    "\n",
    "\n",
    "        #CVI part\n",
    "\n",
    "\n",
    "\n",
    "        cvi_1_dict = {}\n",
    "        cvi_2_dict = {}\n",
    "        cvi_3_dict = {}\n",
    "\n",
    "        for k, v in pop_dict.items():\n",
    "            clustering = v[0].fit(X)\n",
    "\n",
    "            cvi_1_dict[k] = Validation(X, X, clustering.labels_).s_dbw()\n",
    "            cvi_2_dict[k] = davies_bouldin_score(X, clustering.labels_)\n",
    "            cvi_3_dict[k] = banfeld_raferty(X, clustering.labels_)\n",
    "\n",
    "        cvi_1_dict = dict(sorted(cvi_1_dict.items(), key=lambda item: item[1], reverse=False))\n",
    "        cvi_2_dict = dict(sorted(cvi_2_dict.items(), key=lambda item: item[1], reverse=False))\n",
    "        cvi_3_dict = dict(sorted(cvi_3_dict.items(), key=lambda item: item[1], reverse=False))\n",
    "\n",
    "        cvi_1_rank = list(cvi_1_dict.keys())\n",
    "        cvi_2_rank = list(cvi_2_dict.keys())\n",
    "        cvi_3_rank = list(cvi_3_dict.keys())\n",
    "\n",
    "\n",
    "        # Rank configurations by external CVI (NMI) \n",
    "\n",
    "        ground_truth_dict = {}\n",
    "        for k, v in pop_dict.items():\n",
    "            clustering = v[0].fit(X)\n",
    "            ground_truth_dict[k] = normalized_mutual_info_score(y, clustering.labels_)\n",
    "            #print(ground_truth_dict)\n",
    "\n",
    "        ground_truth_dict = dict(sorted(ground_truth_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        gt_cvi_rank = list(ground_truth_dict.keys())\n",
    "        #print(gt_cvi_rank)\n",
    "        cvi_1_correlation = spearmanr(cvi_1_rank, gt_cvi_rank)\n",
    "        cvi_2_correlation = spearmanr(cvi_2_rank, gt_cvi_rank)\n",
    "        cvi_3_correlation = spearmanr(cvi_3_rank, gt_cvi_rank)\n",
    "        multi_correlation = spearmanr(multi_cvi_rank, gt_cvi_rank)\n",
    "\n",
    "        #print(\"Correlation with SDbw: \", cvi_1_correlation.correlation)\n",
    "        #print(\"Correlation with Davies Bouldin Score: \", cvi_2_correlation.correlation)\n",
    "        #print(\"Correlation with Banfeld Raferty: \", cvi_3_correlation.correlation)\n",
    "        #print(\"Original NMI\", gt_cvi_rank)\n",
    "        #print(\"Correlation with multi CVI: \", multi_correlation.correlation)\n",
    "        #print(meta_dict)\n",
    "        \n",
    "        #Finding out the categorical columns\n",
    "        cat_cols=df.iloc[:, :-1].columns\n",
    "        cat_num_cols = df.iloc[:, :-1]._get_numeric_data().columns\n",
    "\n",
    "         #print(gt_cvi_rank)\n",
    "        \n",
    "        #pushing everythign to the dataframe to create the knowledge base\n",
    "        df_meta=df_meta.append({'name': new_file, \n",
    "                             'instances':df.shape[0], \n",
    "                             'attributtes':df.shape[1]-1, \n",
    "                             'classes':df['class'].nunique(), \n",
    "\n",
    "                             'cat_att':len(list(set(cat_cols) - set(cat_num_cols))),\n",
    "                             'cont_att':len(num_cols),\n",
    "                             #'multi_CVI': key,\n",
    "                             #'score':value, \n",
    "                            'mean': meta_dict.get('mean')[0],\n",
    "                            'sd': meta_dict.get('sd')[0],\n",
    "                            'var': meta_dict.get('var')[0],\n",
    "                            'kurtosis': meta_dict.get('kurtosis')[0],\n",
    "                            'skewness': meta_dict.get('skewness')[0],\n",
    "                            'MD6': meta_dict.get('MD6')[0],\n",
    "                            'MD7': meta_dict.get('MD7')[0],\n",
    "                                'MD8': meta_dict.get('MD8')[0],\n",
    "                                'MD9': meta_dict.get('MD9')[0],\n",
    "                                'MD10': meta_dict.get('MD10')[0],\n",
    "                                'MD11': meta_dict.get('MD11')[0],\n",
    "                                'MD12': meta_dict.get('MD12')[0],\n",
    "                                'MD13': meta_dict.get('MD13')[0],\n",
    "                                'MD14': meta_dict.get('MD14')[0],\n",
    "                                'MD15': meta_dict.get('MD15')[0],\n",
    "                                'MD16': meta_dict.get('MD16')[0],\n",
    "                                'MD17': meta_dict.get('MD17')[0],\n",
    "                                'MD18': meta_dict.get('MD18')[0],\n",
    "                                'MD19': meta_dict.get('MD19')[0],\n",
    "                                'best_cluster_setting':pop_dict[multi_cvi_rank[0]],\n",
    "                                'transformation':str(\"mean + \"+str(t)),\n",
    "                                'multi_cvi':'banfeld_raferty, davies_bouldin_score, SDbw',\n",
    "\n",
    "\n",
    "                                'multi_cvi_correlation_score': multi_correlation.correlation}, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "df_meta.to_csv(\"transformation_birch_mean.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
